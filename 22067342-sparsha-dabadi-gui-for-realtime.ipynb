{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import threading\n",
    "import tkinter as tk\n",
    "import time\n",
    "from tkinter import Label, Button, Frame, Canvas\n",
    "from PIL import Image, ImageTk\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Loading the model\n",
    "json_file = open(\"emotiondetectionmodel.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "\n",
    "model.load_weights(\"emotiondetectionmodel.h5\")\n",
    "\n",
    "# Defining haarcascades for face recognition\n",
    "haar_file = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "face_cascade = cv2.CascadeClassifier(haar_file)\n",
    "\n",
    "labels = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n",
    "emotion_colors = {\n",
    "    'angry': (0, 0, 255),      # Red\n",
    "    'disgust': (0, 128, 0),    # Green\n",
    "    'fear': (128, 0, 128),     # Purple\n",
    "    'happy': (0, 255, 255),    # Yellow\n",
    "    'neutral': (255, 255, 255), # White\n",
    "    'sad': (128, 128, 0),      # Blue\n",
    "    'surprise': (0, 255, 0)    # Light Green\n",
    "}\n",
    "\n",
    "prediction_active = True\n",
    "current_emotion = \"None\"\n",
    "face_detected_time = None\n",
    "stabilization_period = 3\n",
    "is_stabilizing = False\n",
    "face_roi = None\n",
    "confidence_score = 0\n",
    "keypoints = [] \n",
    "\n",
    "def extract_features(image):\n",
    "    feature = np.array(image)\n",
    "    feature = feature.reshape(1, 48, 48, 1)\n",
    "    return feature / 255.0\n",
    "\n",
    "def end_application():\n",
    "    root.quit()\n",
    "\n",
    "def video_stream():\n",
    "    global cap, label_text, current_emotion, face_roi, face_detected_time, is_stabilizing, confidence_score\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        original_frame = frame.copy()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        face_detected = False\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            face_detected = True\n",
    "            face_roi = gray[y:y+h, x:x+w]\n",
    "            \n",
    "            # Draw rectangle around face\n",
    "            if is_stabilizing:\n",
    "                # Create pulsing rectangle during stabilization\n",
    "                pulse_value = int(255 * (np.sin(time.time() * 5) * 0.3 + 0.7))\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, pulse_value, pulse_value), 3)\n",
    "                \n",
    "                # Add progress bar for stabilization\n",
    "                progress = min(1.0, (time.time() - face_detected_time) / stabilization_period)\n",
    "                bar_width = int(w * progress)\n",
    "                cv2.rectangle(frame, (x, y+h+5), (x+bar_width, y+h+15), (0, 255, 255), -1)\n",
    "                cv2.rectangle(frame, (x, y+h+5), (x+w, y+h+15), (100, 100, 100), 1)\n",
    "            else:\n",
    "                # Draw emotion-colored rectangle after prediction\n",
    "                if current_emotion in emotion_colors:\n",
    "                    color = emotion_colors[current_emotion]\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), color, 4)\n",
    "                else:\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            \n",
    "            # Add visualization of facial keypoints \n",
    "            # For demonstration, we'll just show some sample keypoints on the right of the face\n",
    "            if not is_stabilizing and current_emotion != \"None\":\n",
    "                # Create a blue box for the keypoint visualization\n",
    "                box_width = 150\n",
    "                box_height = 150\n",
    "                box_x = x + w + 10\n",
    "                box_y = y\n",
    "                \n",
    "                # If there's no space on the right, place the box elsewhere\n",
    "                if box_x + box_width > frame.shape[1]:\n",
    "                    box_x = x - box_width - 10\n",
    "                    if box_x < 0:\n",
    "                        box_x = frame.shape[1] - box_width - 10\n",
    "                        box_y = y + h + 10\n",
    "                \n",
    "                # Draw blue rectangle for keypoints visualization\n",
    "                cv2.rectangle(frame, (box_x, box_y), (box_x + box_width, box_y + box_height), (255, 0, 0), 2)\n",
    "                \n",
    "                # Draw some keypoints and connections based on the emotion\n",
    "                # This is just for demonstration - real facial landmarks would be different\n",
    "                for i in range(5):\n",
    "                    point_x = box_x + 30 + i * 20\n",
    "                    point_y = box_y + 30\n",
    "                    cv2.circle(frame, (point_x, point_y), 5, (0, 0, 255), -1)\n",
    "                    \n",
    "                    point_x2 = box_x + 30 + i * 20\n",
    "                    point_y2 = box_y + 70\n",
    "                    cv2.circle(frame, (point_x2, point_y2), 5, (0, 0, 255), -1)\n",
    "                    \n",
    "                    cv2.line(frame, (point_x, point_y), (point_x2, point_y2), (255, 255, 255), 1)\n",
    "                    \n",
    "                    if i < 4:\n",
    "                        cv2.line(frame, (point_x, point_y), (point_x + 20, point_y), (255, 255, 255), 1)\n",
    "                        cv2.line(frame, (point_x2, point_y2), (point_x2 + 20, point_y2), (255, 255, 255), 1)\n",
    "            \n",
    "            current_time = time.time()\n",
    "            if face_detected_time is None:\n",
    "                face_detected_time = current_time\n",
    "                is_stabilizing = True\n",
    "                label_text.set(f\"Stabilizing: Please hold position for {stabilization_period} seconds\")\n",
    "            \n",
    "            elapsed_time = current_time - face_detected_time\n",
    "            remaining_time = max(0, stabilization_period - elapsed_time)\n",
    "            \n",
    "            if is_stabilizing:\n",
    "                label_text.set(f\"Stabilizing: Please hold position for {remaining_time:.1f} seconds\")\n",
    "                \n",
    "                # Enhanced countdown text with background\n",
    "                countdown_text = f\"Hold for: {remaining_time:.1f}s\"\n",
    "                text_size = cv2.getTextSize(countdown_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n",
    "                cv2.rectangle(frame, (x, y-30), (x + text_size[0] + 10, y-5), (0, 0, 0), -1)\n",
    "                cv2.putText(frame, countdown_text, (x+5, y-10),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                \n",
    "                if elapsed_time >= stabilization_period and prediction_active:\n",
    "                    is_stabilizing = False\n",
    "                    \n",
    "                    try:\n",
    "                        resized_face = cv2.resize(face_roi, (48, 48))\n",
    "                        img = extract_features(resized_face)\n",
    "                        pred = model.predict(img)\n",
    "                        \n",
    "                        predicted_idx = pred.argmax()\n",
    "                        confidence_score = float(pred[0][predicted_idx])\n",
    "                        prediction_label = labels[predicted_idx]\n",
    "                        current_emotion = prediction_label\n",
    "                        \n",
    "                        # Update prediction text\n",
    "                        label_text.set(f\"Detected: {prediction_label.upper()} ({confidence_score:.2f})\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing face: {e}\")\n",
    "                        label_text.set(f\"Error: {str(e)[:30]}...\")\n",
    "            else:\n",
    "                if current_emotion != \"None\":\n",
    "                    # Enhanced emotion display with background\n",
    "                    result_text = f\"{current_emotion.upper()} ({confidence_score:.2f})\"\n",
    "                    text_size = cv2.getTextSize(result_text, cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, 2)[0]\n",
    "                    text_bg_color = emotion_colors.get(current_emotion, (255, 0, 0))\n",
    "                    cv2.rectangle(frame, (x, y-40), (x + text_size[0] + 10, y-5), (0, 0, 0), -1)\n",
    "                    cv2.putText(frame, result_text, (x+5, y-15), \n",
    "                               cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, text_bg_color, 2)\n",
    "        \n",
    "        if not face_detected:\n",
    "            face_detected_time = None\n",
    "            is_stabilizing = False\n",
    "            if prediction_active:\n",
    "                label_text.set(\"No face detected\")\n",
    "                current_emotion = \"None\"\n",
    "        \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(frame_rgb)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        lbl.imgtk = imgtk\n",
    "        lbl.configure(image=imgtk)\n",
    "    \n",
    "    root.after(10, video_stream)\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Emotion Recognition\")\n",
    "root.configure(bg=\"#1e56a0\")  # Royal blue background \n",
    "\n",
    "# Set window size\n",
    "window_width = 1200\n",
    "window_height = 700\n",
    "screen_width = root.winfo_screenwidth()\n",
    "screen_height = root.winfo_screenheight()\n",
    "x = (screen_width - window_width) // 2\n",
    "y = (screen_height - window_height) // 2\n",
    "root.geometry(f\"{window_width}x{window_height}+{x}+{y}\")\n",
    "\n",
    "# Create a two-column layout\n",
    "main_frame = Frame(root, bg=\"#1e56a0\")\n",
    "main_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=20)\n",
    "\n",
    "# Left column for instructions\n",
    "left_column = Frame(main_frame, bg=\"#1e56a0\", width=400)\n",
    "left_column.pack(side=tk.LEFT, fill=tk.BOTH, padx=(0, 10))\n",
    "left_column.pack_propagate(False)  # Prevent shrinking\n",
    "\n",
    "# App title\n",
    "title_label = Label(left_column, text=\"Expression Recognition\", \n",
    "                   font=(\"Arial\", 28, \"bold\"), bg=\"#1e56a0\", fg=\"white\")\n",
    "title_label.pack(pady=(10, 20))\n",
    "\n",
    "subtitle_label = Label(left_column, text=\"How to use EmotionWave\", \n",
    "                      font=(\"Arial\", 22, \"bold\"), bg=\"#1e56a0\", fg=\"white\")\n",
    "subtitle_label.pack(pady=(0, 20))\n",
    "\n",
    "# Instructions \n",
    "instructions = [\n",
    "    \"1. Position Your Face inside the frame located at the top right of your screen.\",\n",
    "    \"2. Ensure good lighting conditions for better recognition accuracy.\",\n",
    "    \"3. Maintain an ideal distance from your camera (similar to your regular sitting position with a laptop).\",\n",
    "    \"4. Show the Facial Expression that you would like the system to predict.\",\n",
    "    \"5. Hold Your Face Steady for 5 seconds while the system stabilizes.\",\n",
    "    \"6. The system will predict your emotion after the stabilization period.\",\n",
    "    \"7. If the prediction is incorrect, please click the Incorrect Button to report it.\",\n",
    "    \"8. To finish, press the End Button to return to the previous screen.\"\n",
    "]\n",
    "\n",
    "for instr in instructions:\n",
    "    instruction_label = Label(left_column, text=instr, justify=tk.LEFT, wraplength=380,\n",
    "                            font=(\"Arial\", 14), bg=\"#1e56a0\", fg=\"white\")\n",
    "    instruction_label.pack(anchor=tk.W, pady=8)\n",
    "\n",
    "# Right column for video and prediction\n",
    "right_column = Frame(main_frame, bg=\"#1e56a0\")\n",
    "right_column.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Video display area\n",
    "video_frame = Frame(right_column, bg=\"#1e56a0\", bd=2, relief=tk.RIDGE)\n",
    "video_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "\n",
    "lbl = Label(video_frame, bg=\"black\")\n",
    "lbl.pack(padx=5, pady=5, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Prediction output section\n",
    "prediction_frame = Frame(right_column, bg=\"#1e56a0\", height=100)\n",
    "prediction_frame.pack(fill=tk.X, padx=10, pady=10)\n",
    "\n",
    "prediction_label = Label(prediction_frame, text=\"Predicted Output:\", \n",
    "                        font=(\"Arial\", 18, \"bold\"), bg=\"#1e56a0\", fg=\"white\")\n",
    "prediction_label.pack(pady=(10, 5))\n",
    "\n",
    "# Status text box with white background for prediction\n",
    "label_text = tk.StringVar()\n",
    "label_text.set(\"Waiting for face detection...\")\n",
    "label = Label(prediction_frame, textvariable=label_text, font=(\"Arial\", 16), \n",
    "             bg=\"white\", fg=\"black\", width=30, height=2)\n",
    "label.pack(pady=5)\n",
    "\n",
    "# Button frame at bottom\n",
    "button_frame = Frame(right_column, bg=\"#1e56a0\")\n",
    "button_frame.pack(fill=tk.X, padx=10, pady=10)\n",
    "\n",
    "# End button\n",
    "end_btn = Button(button_frame, text=\"END\", font=(\"Arial\", 14, \"bold\"),\n",
    "                width=10, height=1, bg=\"#e94560\", fg=\"white\", command=end_application)\n",
    "end_btn.pack(side=tk.RIGHT, padx=5, pady=10)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Start video stream\n",
    "video_stream()\n",
    "\n",
    "# Keyboard shortcuts\n",
    "def exit_program(event):\n",
    "    root.quit()\n",
    "\n",
    "root.bind(\"<KeyPress-q>\", exit_program)  \n",
    "\n",
    "# Start the main loop\n",
    "root.mainloop()\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
